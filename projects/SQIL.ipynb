{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDSU_zXGDAu"
      },
      "source": [
        "# Имитационное обучение:  алгоритм SQIL.\n",
        "\n",
        "Научиться имитировать поведение экспертов из демонстраций может быть непросто, особенно в средах с многомерными непрерывными наблюдениями и неизвестной динамикой. Методы копирования поведения, страдают от сдвига распределения (distribution shift): поскольку агент жадно имитирует продемонстрированные действия, он может отклоняться от продемонстрированных состояний, что приводит к накоплению ошибок.\n",
        "\n",
        "Оригинальная статья: [SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards](https://arxiv.org/abs/1905.11108)\n",
        "\n",
        "В данной части мы попробуем применить идею из этой статьи для среды ``LunarLanderContinuous-v2``, воспользовавшись кодом алгоритма DDPG с семинара."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLO7IRJuhAQJ"
      },
      "outputs": [],
      "source": [
        "# @title Установка зависимостей\n",
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !apt -qq update -y\n",
        "    !apt -qq install swig -y\n",
        "    !pip -q install box2d-py\n",
        "    !pip -q install \"gymnasium[classic-control, box2d, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZPVELn-GDAu"
      },
      "outputs": [],
      "source": [
        "# @title Импортирование зависимостей\n",
        "import math\n",
        "import random\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# библиотеки и функции, которые потребуеются для показа видео\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "def show_video(folder=\"./video\"):\n",
        "    mp4list = glob.glob(folder + '/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def show_progress(rewards_batch, log, reward_range=None):\n",
        "    \"\"\"\n",
        "    Удобная функция, которая отображает прогресс обучения.\n",
        "    \"\"\"\n",
        "\n",
        "    if reward_range is None:\n",
        "        reward_range = [-990, +10]\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    log.append([mean_reward])\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpRWeOaSS4JN"
      },
      "source": [
        "# Нормализация действия и добавление шума:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkrMry8US3l2"
      },
      "outputs": [],
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "\n",
        "    def action(self, action):\n",
        "        low_bound = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        # [L, M, R]\n",
        "        # actions are in [-1, 1]\n",
        "        # Нормализуйте действия\n",
        "        ####### Здесь ваш код ########\n",
        "        action = \n",
        "        ##############################\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GaussNoise:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def get_action(self, action):\n",
        "        # добавьте нормальный шум\n",
        "        ####### Здесь ваш код ########\n",
        "        noisy_action = \n",
        "        ##############################\n",
        "        return noisy_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESkyAhamS6ps"
      },
      "source": [
        "# Value и Policy сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMnBtE_lS7vy"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_inputs,\n",
        "            num_actions,\n",
        "            hidden_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # добавьте 3 линейных слой (не забудьте про функцию активации Tanh)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.net = \n",
        "        ##############################\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_inputs,\n",
        "            num_actions,\n",
        "            hidden_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # определяем граф вычисления для Policy Network\n",
        "        # добавьте 3 линейных слой (не забудьте про функцию активации Tanh)\n",
        "        ####### Здесь ваш код ########\n",
        "        self.net =\n",
        "        ##############################\n",
        "\n",
        "    def forward(self, state):\n",
        "        # определяем прямой проход по графу вычислений\n",
        "        # x =\n",
        "        x = state\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        функция для выбора действия\n",
        "        \"\"\"\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        action = self.forward(state)\n",
        "        action = action.detach().cpu().numpy()[0]\n",
        "        action = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz8GYof-S8gr"
      },
      "source": [
        "# DDPG обновление\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg\">\n",
        "\n",
        "Оригинальная статья:  <a href=\"https://arxiv.org/abs/1509.02971\">Continuous control with deep reinforcement learning Arxiv</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG2548uTS-WL"
      },
      "outputs": [],
      "source": [
        "def ddpg_update(\n",
        "        state,\n",
        "        action,\n",
        "        reward,\n",
        "        next_state,\n",
        "        done,\n",
        "        gamma=0.99,\n",
        "        min_value=-np.inf,\n",
        "        max_value=np.inf,\n",
        "        soft_tau=0.001,\n",
        "):\n",
        "    state = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
        "    action = torch.tensor(action, dtype=torch.float32).to(device)\n",
        "    reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "    done = torch.tensor(np.float32(done)).unsqueeze(1).to(device)\n",
        "\n",
        "    # считаем policy loss по формуле выше, используя value_net и policy_net\n",
        "    ####### Здесь ваш код ########\n",
        "    policy_loss = \n",
        "    ##############################\n",
        "\n",
        "    next_action = target_policy_net(next_state)\n",
        "    target_value = target_value_net(next_state, next_action.detach())\n",
        "    # считаем таргет Q функцию\n",
        "    ####### Здесь ваш код ########\n",
        "    expected_value = \n",
        "    ##############################\n",
        "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
        "\n",
        "    value = value_net(state, action)\n",
        "    value_loss = nn.MSELoss()(value, expected_value.detach())\n",
        "\n",
        "\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    value_optimizer.step()\n",
        "\n",
        "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "        )\n",
        "\n",
        "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv9a_CeQTAO_"
      },
      "source": [
        "# Стандартная и комбинированная память прецедентов:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyIcEo8US_GS"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class CombinedReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.demo = ReplayBuffer(self.capacity)\n",
        "        self.agent = ReplayBuffer(self.capacity)\n",
        "\n",
        "    def push_demo(self, state, action, reward, next_state, smart_done):\n",
        "        # модифицируем вознаграждения, как этого предусматривает алгоритм SQIL\n",
        "        # reward =\n",
        "        ####### Здесь ваш код ########\n",
        "        reward =\n",
        "        ##############################\n",
        "        self.demo.push(state, action, reward, next_state, smart_done)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, smart_done):\n",
        "        # модифицируем вознаграждения, как этого предусматривает алгоритм SQIL\n",
        "        # reward =\n",
        "        ####### Здесь ваш код ########\n",
        "        reward = \n",
        "        ##############################\n",
        "        self.agent.push(state, action, reward, next_state, smart_done)\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        demo_batch_size = min(batch_size // 2, len(self.demo))\n",
        "        # набираем данные из обоих буферов (семплирование из буферов)\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        ##############################\n",
        "\n",
        "        return np.concatenate([states, demo_states]), \\\n",
        "               np.concatenate([actions, demo_actions]), \\\n",
        "               np.concatenate([rewards, demo_rewards]), \\\n",
        "               np.concatenate([next_states, demo_next_states]), \\\n",
        "               np.concatenate([dones, demo_dones])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.demo) + len(self.agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk7is64CTCQH"
      },
      "source": [
        " # Метод ``generate_session``\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IyL6wkFTDuY"
      },
      "outputs": [],
      "source": [
        "def generate_session(train=False):\n",
        "    \"\"\"эпизод взаимодействие агента со средой, а также вызов процесса обучения\"\"\"\n",
        "    total_reward = 0\n",
        "    state, info = env.reset()\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy_net.get_action(state)\n",
        "        if train:\n",
        "            action = noise.get_action(action)\n",
        "        next_state, reward, term, trunc, info = env.step(action)\n",
        "        done = term or trunc\n",
        "        if train:\n",
        "            replay_buffer.push(state, action, reward, next_state, term)\n",
        "            if len(replay_buffer) > replay_buffer_size + 1500:\n",
        "                ddpg_update(*replay_buffer.sample(batch_size))\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YB2qXtyTGPZ"
      },
      "source": [
        "# Задание гиперпараметров и инициализация всего и вся:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZOPhbttTE9P"
      },
      "outputs": [],
      "source": [
        "env_name = \"LunarLanderContinuous-v3\"\n",
        "\n",
        "max_steps = 350\n",
        "env = NormalizedActions(gym.make(env_name, max_episode_steps=max_steps))\n",
        "\n",
        "noise = GaussNoise(sigma=0.1)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dim = 512\n",
        "\n",
        "value_net = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "target_value_net = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "\n",
        "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
        "    target_param.data.copy_(param.data)\n",
        "\n",
        "value_lr = 1e-3\n",
        "policy_lr = 1e-4\n",
        "\n",
        "value_optimizer = optim.Adam(value_net.parameters(), lr=value_lr, weight_decay=1e-6)\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr, weight_decay=1e-6)\n",
        "\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMDZHwpTISu"
      },
      "source": [
        "### Генерация экспертных данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZBM4JsLTJH4"
      },
      "outputs": [],
      "source": [
        "from gymnasium.envs.box2d.lunar_lander import heuristic\n",
        "\n",
        "replay_buffer_size = 100000\n",
        "replay_buffer = CombinedReplayBuffer(replay_buffer_size)\n",
        "\n",
        "noise = GaussNoise(sigma=0.1)\n",
        "episodes = 0\n",
        "while len(replay_buffer) < replay_buffer_size:\n",
        "    episodes += 1\n",
        "    done = False\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        action = noise.get_action(heuristic(env, state))\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        replay_buffer.push_demo(state, action, reward, next_state, terminated)\n",
        "        done = terminated or truncated\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "    if episodes % 100 == 0:\n",
        "        print(f\"episode: {episodes}, reward: {episode_reward}, replay_size:\", len(replay_buffer))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8yn3DcdTK7W",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "env = NormalizedActions(gym.make(env_name, max_episode_steps=max_steps))\n",
        "noise = GaussNoise(sigma=0.001)\n",
        "valid_mean_rewards = []\n",
        "for i in range(100):\n",
        "    session_rewards_train = [generate_session(train=True) for _ in range(10)]\n",
        "\n",
        "    mean_reward = np.mean(session_rewards_train)\n",
        "    print(f\"epoch #{i:02d}\\tmean reward (train) = {mean_reward:.3f}\\t\")\n",
        "\n",
        "    if mean_reward > 200:\n",
        "        print(\"Выполнено!\")\n",
        "        break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjyY2GHPTMRy"
      },
      "source": [
        "# Посмотрим за полетом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91RCFUMkTPGT",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "env = NormalizedActions(gym.make(env_name, max_episode_steps=max_steps, render_mode='rgb_array'))\n",
        "env = RecordVideo(env, f\"./video\")\n",
        "\n",
        "done = False\n",
        "\n",
        "state, info = env.reset()\n",
        "\n",
        "while not done:\n",
        "    action = policy_net.get_action(state)\n",
        "    state, _, term, trunc, info = env.step(action)\n",
        "    done = term or trunc\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmo4nY1s0F0Y"
      },
      "source": [
        "# Загрузка датасета MuJoCo\n",
        "\n",
        "Загрузите датасет для одной из сред MuJoCo. Используйте [данные](https://minari.farama.org/main/datasets/mujoco/) от Minari."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-0_PJUaVFwg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7uuwW008O9"
      },
      "source": [
        "# Обучение на загруженных данных\n",
        "\n",
        "Повторите обучение алгоритма SQIL на загруженном датасете. Необходимо обучить алгоритм на каждом из вариантов датасета (simple, medium, expert).\n",
        "\n",
        "Проанализируйте полученные результаты обучения на каждом из датасетов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
