{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Pp7oegpV2Dr2",
      "metadata": {
        "id": "Pp7oegpV2Dr2"
      },
      "source": [
        "# Семинар 4. Глубокое Q обучение\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd",
      "metadata": {
        "id": "8e2ad6ff-2b99-457b-9b1c-68d631b30acd"
      },
      "outputs": [],
      "source": [
        "# @title Установка зависимостей\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49673813",
      "metadata": {
        "id": "49673813"
      },
      "outputs": [],
      "source": [
        "# @title Импортирование зависимостей\n",
        "\n",
        "import time\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque\n",
        "\n",
        "def show_video(folder=\"./video\"):\n",
        "    mp4list = glob.glob(folder + '/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def show_progress(rewards_batch, log):\n",
        "    \"\"\"\n",
        "    Удобная функция, которая отображает прогресс обучения.\n",
        "    \"\"\"\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    log.append(mean_reward)\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(log, label='Mean rewards')\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()\n",
        "\n",
        "def video_episode(env_name, policy=None):\n",
        "    # создаем среду с ограничением на число шагов в среде\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\", max_episode_steps=250)\n",
        "    # добавляем визуализацию\n",
        "    env = RecordVideo(env, f\"./video\", name_prefix=str(int(time.time())))\n",
        "\n",
        "    # проводим инициализацию и запоминаем начальное состояние\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # выполняем действие, получаем s, r, term, trunc, info\n",
        "        if policy:\n",
        "            s, r, terminated, truncated, _ = env.step(policy(s))\n",
        "        else:\n",
        "            s, r, terminated, truncated, _ = env.step(env.action_space.sample())\n",
        "        done = terminated or truncated\n",
        "\n",
        "    env.close()\n",
        "    show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KI2H1r25AdWl",
      "metadata": {
        "id": "KI2H1r25AdWl"
      },
      "source": [
        "# 1. Основы PyTorch\n",
        "Будем использовать библиотеку [pytorch](https://pytorch.org/) для обучения нейронных сетей, хотя можно использовать и любую другую библиотеку (например, [tensorflow](https://www.tensorflow.org/), [flax](https://github.com/google/flax), [equinox](https://docs.kidger.site/equinox/)).\n",
        "\n",
        "Рассмотрим простую задачу линейной регрессии. Для обучения с подкреплением она является основой для обучения функции полезности.\n",
        "\n",
        "Линейная регрессия: восстановить параметры функции на основе экспериментальных данных.\n",
        "\n",
        "Общий вид линейной функции:\n",
        "$$y = ax + b + \\epsilon,$$\n",
        "где $a, b$ - параметры, а $\\epsilon$ - некоторый шум.\n",
        "\n",
        "Экспериментальные данные (датасет) - набор пар: аргумент-значение, для нашего примера это $(x, y)$. Решая задачу линейной регрессии, мы будем искать параметры, которые позволяют задать отображение $f_{a,b}(x) = ax + b$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WGFKjDhlDv3Y",
      "metadata": {
        "id": "WGFKjDhlDv3Y"
      },
      "outputs": [],
      "source": [
        "a = 1\n",
        "b = 2\n",
        "\n",
        "# генерация данных\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(100, 1)\n",
        "y = a + b * x + 0.1 * np.random.randn(100, 1)\n",
        "\n",
        "# Перемещиваем индексы\n",
        "idx = np.arange(100)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# Разделяем на тренировочную и валидационную выборки (чисто для демонстрационных целей)\n",
        "train_idx = idx[:80]\n",
        "val_idx = idx[80:]\n",
        "\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AzsQuF0REKRa",
      "metadata": {
        "id": "AzsQuF0REKRa"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FluiOJwxEzwm",
      "metadata": {
        "id": "FluiOJwxEzwm"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Наши данные это numpy массивы, необходимо их перенести на вычислительное\n",
        "# устройство и преобразовать в тензоры торча\n",
        "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
        "\n",
        "# Посмотрим разницу между объектами\n",
        "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iUbyCEGZ2efz",
      "metadata": {
        "id": "iUbyCEGZ2efz"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aCt31F4dFNeJ",
      "metadata": {
        "id": "aCt31F4dFNeJ"
      },
      "outputs": [],
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Строим функцию, осуществяющую обучение\n",
        "    def train_step(x, y):\n",
        "        # Выставляем TRAIN режим\n",
        "        model.train()\n",
        "        # Step 1: Делаем предсказания\n",
        "        yhat = model(x)\n",
        "        # Step 2: Вычисляем функцию потерь\n",
        "        loss = loss_fn(y, yhat)\n",
        "        # Step 3: Вычисляем градиенты\n",
        "        loss.backward()\n",
        "        # Step 4: Обновляем параметры и стираем градиенты\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    # Возвращаем функцию обучения\n",
        "    return train_step\n",
        "\n",
        "# Начинаем обучение с нуля\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Задаем функцию для обучения\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Один шаг обучения\n",
        "    loss = train_step(x_train_tensor, y_train_tensor)\n",
        "    losses.append(loss)\n",
        "\n",
        "# Проверим параметры модели\n",
        "print(model.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cInOetXmE8UP",
      "metadata": {
        "id": "cInOetXmE8UP"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sE2wzOmQGVJD",
      "metadata": {
        "id": "sE2wzOmQGVJD"
      },
      "outputs": [],
      "source": [
        "x_true = np.linspace(0, 1, 100)\n",
        "y_true = a + b * x_true\n",
        "\n",
        "plt.plot(x_true, y_true, label='True')\n",
        "plt.scatter(x_train, y_train)\n",
        "y_pred = model(torch.from_numpy(x_true[:, None]).float().to(device)).cpu().detach().numpy()\n",
        "plt.plot(x_true, y_pred, label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U39didHd2MpJ",
      "metadata": {
        "id": "U39didHd2MpJ"
      },
      "source": [
        "# 2. Deep Q Network <a name = 'dqn'></a>\n",
        "\n",
        "Зачем в обучении с подкреплением использовать аппроксиматоры?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pYNULO4r42ZB",
      "metadata": {
        "id": "pYNULO4r42ZB"
      },
      "source": [
        "Рассмотрим среду [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/). На данном этапе мы ограничены, и можем использовать только дискретное пространство действий.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" width=500/>\n",
        "\n",
        "## **Задание**\n",
        "Откройте среды [Classic control](https://gymnasium.farama.org/environments/classic_control/), какие из них подходит под условие использования дискретных действий?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tDLuoqvY30Ec",
      "metadata": {
        "id": "tDLuoqvY30Ec"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(f'Action_space: {n_actions} \\nState_space: {state_dim}')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g7cPGwuC5Zii",
      "metadata": {
        "id": "g7cPGwuC5Zii"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\" width=500>\n",
        "\n",
        "## **Вопрос**\n",
        "\n",
        "В чем преимущество использования глубоких нейронных сетей?\n",
        "\n",
        "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``).\n",
        "\n",
        "## **Задание**\n",
        "\n",
        "Допишите код, инициализирующий полносвязную нейронную сеть с 3-мя скрытыми слоями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W9F8-Y6wUeVa",
      "metadata": {
        "id": "W9F8-Y6wUeVa"
      },
      "outputs": [],
      "source": [
        "def create_network(input_dim, hidden_dims: list[int], output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU(),\n",
        "    #    ...\n",
        "    # )\n",
        "    # add 3 layers!\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2SP9HER6C7n",
      "metadata": {
        "id": "d2SP9HER6C7n"
      },
      "source": [
        "## Задание\n",
        "Реализуйте эпсилон-жадную стратегию для Q-функции, определенной через нейронную сеть."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfsvHTWrUeVb",
      "metadata": {
        "id": "vfsvHTWrUeVb"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(network, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = network(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T8GxbUHPAldO",
      "metadata": {
        "id": "T8GxbUHPAldO"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "video_episode(\"CartPole-v1\", lambda x: select_action_eps_greedy(network, x, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oZgNdCIFUeVV",
      "metadata": {
        "id": "oZgNdCIFUeVV"
      },
      "source": [
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{target}(s, a) - Q_{\\theta}(s, a) = [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] - Q_{\\theta}(s, a) \\\\\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние,\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0rmIcveUeVb",
      "metadata": {
        "id": "b0rmIcveUeVb"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(\n",
        "        network, states, actions, rewards, next_states, is_done, gamma=0.99,\n",
        "        check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
        "\n",
        "\n",
        "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
        "\n",
        "    # получаем значения Q(s, ...) для всех действий из текущих состояний\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # получаем Q(s, a) для выбранных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # применяем сеть для получения Q(s', ...) для следующих состояний (next_states)\n",
        "    # predicted_next_qvalues =\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    # ВАЖНО!!! Необходимо остановить градиенты - открепить значения\n",
        "    # от графа вычислений. Можно использовать .detach()\n",
        "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
        "    # next_state_values =\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # вычисляем target q-values для функции потерь\n",
        "    #  target_qvalues_for_actions =\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    # для последнего действия в эпизоде используем\n",
        "    # упрощенную формулу Q(s,a) = r(s,a),\n",
        "    # т.к. s' для него не существует\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
        "    loss = torch.mean(losses)\n",
        "    # добавляем регуляризацию на значения Q\n",
        "    loss += regularizer * torch.abs(predicted_qvalues_for_actions).mean()\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3mXlBwqNTox3",
      "metadata": {
        "id": "3mXlBwqNTox3"
      },
      "source": [
        "## **Задание**\n",
        "\n",
        "Что нужно изменить, чтобы получить SARSA?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jpnzZ2Mo9gXC",
      "metadata": {
        "id": "jpnzZ2Mo9gXC"
      },
      "source": [
        "Объединяем все в цикл взаимодействия агента со средой и обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DU-PTCmKUeVd",
      "metadata": {
        "id": "DU-PTCmKUeVd"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ObLjKGCUeVd",
      "metadata": {
        "id": "_ObLjKGCUeVd"
      },
      "outputs": [],
      "source": [
        "lr = .0001\n",
        "eps, eps_decay = .5, .998\n",
        "train_ep_len, eval_schedule = 10000, 50\n",
        "eval_rewards = deque(maxlen=5)\n",
        "\n",
        "env.reset()\n",
        "network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "for ep in range(train_ep_len):\n",
        "    _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
        "\n",
        "    if (ep + 1) % eval_schedule == 0:\n",
        "        ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
        "        eval_rewards.append(ep_rew)\n",
        "        running_avg_rew = np.mean(eval_rewards)\n",
        "        print(f\"Epoch: #{ep}\\tmean reward = {running_avg_rew:.3f}\\tepsilon = {eps:.3f}\")\n",
        "\n",
        "        if eval_rewards and running_avg_rew >= 200.:\n",
        "            print(\"Принято!\")\n",
        "            break\n",
        "\n",
        "    eps *= eps_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mpwfJuChcj0B",
      "metadata": {
        "id": "mpwfJuChcj0B"
      },
      "source": [
        "`mean_reward` - это средняя отдача за эпизод на истории из последних 5 эпизодов. В случае корректной реализации, этот показатель будет низким первые 1000 шагов и только затем будет возрастать и сойдется на 5000-15000 шагах в зависимости от архитектуры сети."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K5vN8d6xUT1l",
      "metadata": {
        "id": "K5vN8d6xUT1l"
      },
      "source": [
        "## **Вопрос**\n",
        "Почему используется затухание для $\\epsilon$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CvxuePfG-ls2",
      "metadata": {
        "id": "CvxuePfG-ls2"
      },
      "outputs": [],
      "source": [
        "video_episode(\"CartPole-v1\", lambda x: select_action_eps_greedy(network, x, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3BYIt62uAUav",
      "metadata": {
        "id": "3BYIt62uAUav"
      },
      "source": [
        "# 3. DQN with Experience Replay\n",
        "\n",
        "Теперь добавим поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
        "\n",
        "Во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-AaUvmgDqLX",
      "metadata": {
        "id": "k-AaUvmgDqLX"
      },
      "source": [
        "## Задание\n",
        "\n",
        "Реализуйте алгоритм семплирования из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "luUbqZWMUeVe",
      "metadata": {
        "id": "luUbqZWMUeVe"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dvUuig1oD2zD",
      "metadata": {
        "id": "dvUuig1oD2zD"
      },
      "source": [
        "Добавим в алгоритм взаимодействия агента со средой пополнение памяти прецедентов и обучение на основе выборки данных из памяти."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DusJxUGSUeVe",
      "metadata": {
        "id": "DusJxUGSUeVe"
      },
      "outputs": [],
      "source": [
        "def generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # put new sample into replay_buffer\n",
        "            replay_buffer.append((s, a, r, next_s, terminated))\n",
        "\n",
        "            if replay_buffer and glob_step % train_schedule == 0:\n",
        "                # sample new batch\n",
        "                train_batch = sample_batch(replay_buffer, batch_size)\n",
        "                states, actions, rewards, next_states, is_terminal = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_terminal)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kNXqI5VUEijW",
      "metadata": {
        "id": "kNXqI5VUEijW"
      },
      "source": [
        "Осталось протестировать новый DQN с памятью предентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyUvgduEUeVf",
      "metadata": {
        "id": "vyUvgduEUeVf"
      },
      "outputs": [],
      "source": [
        "lr = .0001\n",
        "eps, eps_decay = .5, .998\n",
        "train_ep_len, eval_schedule = 10000, 50\n",
        "train_schedule, batch_size = 4, 32\n",
        "replay_buffer = deque(maxlen=4000)\n",
        "eval_rewards = deque(maxlen=5)\n",
        "glob_step = 0\n",
        "\n",
        "env.reset()\n",
        "network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "for ep in range(train_ep_len):\n",
        "    _, glob_step = generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "    )\n",
        "\n",
        "    if (ep + 1) % eval_schedule == 0:\n",
        "        ep_rew, _ = generate_session_rb(\n",
        "            env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "        )\n",
        "        eval_rewards.append(ep_rew)\n",
        "        running_avg_rew = np.mean(eval_rewards)\n",
        "        print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "        if eval_rewards and running_avg_rew >= 200.:\n",
        "            print(\"Принято!\")\n",
        "            break\n",
        "\n",
        "    eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xtu6uUQcBzBT",
      "metadata": {
        "id": "xtu6uUQcBzBT"
      },
      "outputs": [],
      "source": [
        "video_episode(\"CartPole-v1\", lambda x: select_action_eps_greedy(network, x, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E9syzgXZeRt1",
      "metadata": {
        "id": "E9syzgXZeRt1"
      },
      "source": [
        "# 4. DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому примеру, хранящемуся в памяти, значение приоритета. Приоритет будет влиять на частоту случайного выбора примеров в пакет на обучение. Удачный выбор приоритета позволит повысить эффективность обучения. Популярным вариантом является абсолютное значение TD-ошибки: акцент при обучении Q-функции отводится примерам, на которых аппроксиматор ошибается сильнее.\n",
        "\n",
        "Необходимо помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз накладно. Из-за этого потребуется искать баланс между точностью оценки приоритета и скоростью работы.\n",
        "\n",
        "В данном задании мы будем делать следующее:\n",
        "\n",
        "- Использовать TD-ошибку в качестве приоритета.\n",
        "- Так как для батча данных, используемых при обучении, в любом случае будет вычислена TD-ошибка, воспользуемся полученными значениями для обновления значений приоритета в памяти для каждого примера из данного батча.\n",
        "- Будем периодически сортировать память для того, чтобы новые добавляемые переходы заменяли собой те переходы, у которых наименьший приоритет (т.е. наименьшие значения ошибки).\n",
        "\n",
        "**Обратите внимание**, что софтмакс очень чувствителен к масштабу величин и часто требует подбора температуры. Чтобы частично нивелировать эту проблему, предлагается использовать не softmax(priorities) напрямую, а воспользоваться функцией\n",
        "$$\\mathrm{symlog}(x) = \\mathrm{sign}(x)\\mathrm{log}(|x| + 1),$$\n",
        "то есть softmax(symlog(priorities)), и не подбирать температуру. Идея взята из статьи [DreamerV3](https://arxiv.org/pdf/2301.04104)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_qwHXdjQe8Rc",
      "metadata": {
        "id": "_qwHXdjQe8Rc"
      },
      "outputs": [],
      "source": [
        "def symlog(x):\n",
        "    \"\"\"\n",
        "    Compute symlog values for a vector `x`.\n",
        "    It's an inverse operation for symexp.\n",
        "    \"\"\"\n",
        "    return np.sign(x) * np.log(np.abs(x) + 1)\n",
        "\n",
        "def softmax(xs, temp=1.):\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` examples from replay buffer\n",
        "    # weighting by priority (example's TD error) and split an array\n",
        "    # of sample tuples into arrays:\n",
        "    #    states, actions, rewards, next_states, terminateds\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    # Note that each sample in replay buffer is a tuple:\n",
        "    #   (priority, state, action, reward, next_state, terminated)\n",
        "    # Use\n",
        "    ####### Здесь ваш код ########\n",
        "\n",
        "    ##############################\n",
        "\n",
        "    batch = (\n",
        "        np.array(states), np.array(actions), np.array(rewards),\n",
        "        np.array(next_states), np.array(terminateds)\n",
        "    )\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_priority):\n",
        "    \"\"\"Updates batches with corresponding indices\n",
        "    replacing their priority values.\"\"\"\n",
        "    states, actions, rewards, next_states, terminateds = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = (\n",
        "            new_priority[i], states[i], actions[i], rewards[i],\n",
        "            next_states[i], terminateds[i]\n",
        "        )\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with\n",
        "    lesser priority to the beginning ==> they will be\n",
        "    replaced with the new samples sooner.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "f4f1d2d8cbda689bee08cb8d7fe5a19f770d75378302676f04af068af2c2973a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
