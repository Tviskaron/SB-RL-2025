{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lGZSpv_ZZMl"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "В практической реализации существует два варианта реализации алгоритма PPO:\n",
        "* выполняет обновление, ограниченное KL, как TRPO, но штрафует KL-расхождение в целевой функции вместо того, чтобы делать его жестким ограничением, и автоматически регулирует коэффициент штрафа в процессе обучения, чтобы он масштабировался соответствующим образом.\n",
        "* не содержит в целевой функции члена KL-дивергенции и вообще не имеет ограничения. Вместо этого полагается на специализированный клиппинг\n",
        "\n",
        "<img src=\"https://spinningup.openai.com/en/latest/_images/math/e62a8971472597f4b014c2da064f636ffe365ba3.svg\">\n",
        "\n",
        "Спойлер: клиппинг - не самое главное в PPO, как это могло показаться на первый взгляд. Алгоритм PPO работает во многом и за счет небольших дополнительных улучшений. Подробнее: https://arxiv.org/pdf/2005.12729.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMBoV3wUZZMp"
      },
      "source": [
        "# Устанавливаем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3t4TxHsZZMq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !apt -qq update -y\n",
        "    !apt -qq install swig -y\n",
        "    !pip -q install box2d-py\n",
        "    !pip -q install \"gymnasium[classic-control, box2d, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85astBbBZZMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SRC_b_9c8yB"
      },
      "source": [
        "# Память"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPunJDSFZZMs"
      },
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bT5JGY_ZZMs"
      },
      "source": [
        "# Сеть Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqYfrssoZZMs"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor: 2 hidden + output\n",
        "        # self.action_layer = nn.Sequential(..., nn.Softmax(dim=-1))\n",
        "        # 3 linear layers\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        ##############################\n",
        "\n",
        "        # critic: 2 hidden + output\n",
        "        # self.value_layer = nn.Sequential(...)\n",
        "        # 3 linear layers\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        ##############################\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        # сохраняем в память: state, action, log_prob(action)\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6sW_qySZZMt"
      },
      "source": [
        "# PPO policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFdZvsDPZZMt"
      },
      "outputs": [],
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def update(self, memory):\n",
        "        # Monte Carlo оценка вознаграждений:\n",
        "        returns = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            # обнуляем накопленную награду, если попали в терминальное состояние\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            # discounted_reward =\n",
        "            ####### Здесь ваш код ########\n",
        "\n",
        "            ##############################\n",
        "            returns.insert(0, discounted_reward)\n",
        "\n",
        "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "        # выполните нормализацию вознаграждений (r - mean(r)) / std(r + 1e-5):\n",
        "        # returns =\n",
        "        ####### Здесь ваш код ########\n",
        "\n",
        "        ##############################\n",
        "\n",
        "        # конвертация list в tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        # оптимизация K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # получаем logprobs, state_values, dist_entropy от стратегии:\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # находим отношение стратегий (pi_theta / pi_theta__old), через logprobs и old_logprobs.detach():\n",
        "            # ratios =\n",
        "            ####### Здесь ваш код ########\n",
        "\n",
        "            ##############################\n",
        "\n",
        "            # считаем advantages\n",
        "            # advantages =\n",
        "            ####### Здесь ваш код ########\n",
        "\n",
        "            ##############################\n",
        "\n",
        "            # Находим surrogate loss:\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, returns) - 0.01 * dist_entropy\n",
        "\n",
        "            # делаем шаг градиента\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # копируем веса\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isnuAIhPZZMu"
      },
      "source": [
        "# Основной цикл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZvRDe11DZZMu"
      },
      "outputs": [],
      "source": [
        "# env_name = \"CartPole-v1\"\n",
        "env_name = \"LunarLander-v3\"\n",
        "# env_name = \"MountainCar-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "render = False\n",
        "solved_reward = 200  # останавливаемся если avg_reward > solved_reward\n",
        "log_interval = 20  # печатаем avg reward  в интервале\n",
        "max_episodes = 50000  # количество эпизодов обучения\n",
        "max_timesteps = 500  # максимальное кол-во шагов в эпизоде\n",
        "n_latent_var = 64  # кол-во переменных в скрытых слоях\n",
        "update_timestep = 2000  # обновляем policy каждые n шагов\n",
        "lr = 0.001 # learning rate\n",
        "betas = (0.9, 0.999) # betas для adam optimizer\n",
        "gamma = 0.99  # discount factor\n",
        "K_epochs = 4  # количество эпох обновления policy\n",
        "eps_clip = 0.1  # clip параметр для PPO\n",
        "random_seed = None\n",
        "\n",
        "if random_seed:\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "print(lr, betas)\n",
        "\n",
        "# переменные для логирования\n",
        "running_reward = 0\n",
        "avg_length = 0\n",
        "timestep = 0\n",
        "\n",
        "# цикл обучения\n",
        "for i_episode in range(1, max_episodes + 1):\n",
        "    state, _ = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        timestep += 1\n",
        "\n",
        "        # используем policy_old для выбора действия\n",
        "        action = ppo.policy_old.act(state, memory)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # сохраняем награды и флаги терминальных состояний:\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(terminated)\n",
        "\n",
        "        # выполняем обновление\n",
        "        if timestep % update_timestep == 0:\n",
        "            ppo.update(memory)\n",
        "            memory.clear_memory()\n",
        "            timestep = 0\n",
        "\n",
        "        running_reward += reward\n",
        "        if render:\n",
        "            env.render()\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    avg_length += t\n",
        "\n",
        "    # останавливаемся, если avg_reward > solved_reward\n",
        "    if running_reward > (log_interval * solved_reward):\n",
        "        print(\"########## Принято! ##########\")\n",
        "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
        "        break\n",
        "\n",
        "    # логирование\n",
        "    if i_episode % log_interval == 0:\n",
        "        avg_length = int(avg_length / log_interval)\n",
        "        running_reward = int((running_reward / log_interval))\n",
        "\n",
        "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "        running_reward = 0\n",
        "        avg_length = 0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}